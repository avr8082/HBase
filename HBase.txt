
HDFS:
1. HDFS can handle large files 
2. Data can be loaded using batch mode 
3. HDFS is not for operational purpose 
4.HDFS is MapReduce based 
5. For ex: in 1 TB data, if 6 records are corrupted, we cannot rectify and load. We have to delete entire dataset and load again 1TB data.
   This kind of operations not possible in HDFS. HDFS is not for operational purpose.

HBase:
1. Data has to be indexed
2. Data has to be on cluster 
3. Region Servers store the data physically
4. Master-slave architecture 
5. HBase uses HDFS as underlying storage 

HBase Modeling types:
Thick data modeling --> column families have many different columns for each row
Thin data modeling --> column family size is smaller for each row

$ hadoop fs -ls /apps 

HBase Architecture components:
SSTable (sorted string table)
WAL 
Memstore 

SSTable will be created based on certain threshold 
Logs are based on file structure.Logs will be preserved. 

WAL:Write Ahead Log 
- before data into SSTable 
- File structured 
- In-memory 

Compaction:
Minor
Major

# START HBASE 
$ hbase shell 
hbase: > create_namespace training;
> help create_namespace

HBase CRUD operations:

> create 'training:emp', 'ed', 'd' # create table in nama space with column family 
> put 'training:emp', 1, 'ed:ename', 'IT Versity'     # loading 1 column for this column 

> scan 'taining:emp'  # it will display record with time stamp 
> put 'training:emp' , 1, 'ed:salary', '0'

> put 'training:emp' , 1, 'ed:address' 'Webex'

> scan 'training:emp'

> put 'training:emp', 1, 'd:dname', 'lms'
> put 'training:emp', 1, 'd:address', 'digital ocean'

When we run the scan, by default it is sorted with ROW KEY

> get 'training:emp', 1 # 1 is row key here & to look up the column values 

> delete 'taining:emp', 1 'ed:salary'  # delete only salary. Delete only one column at a time

> show_filters;



$ cd /etc/hbase/conf 

To drop the database, disable and drop the database. ***

> put 'training:emp', 1, 'ed:it|oracle', 0
> scan 'taining:emp'


> put 'training:emp', '1:oracle', 'ed:en', 'Durga'
> scan 'taining:emp'


> create 'training:eagg', 1, 'cf1:Apache Spark', 37
> create 'training:eagg', 1, 'cf1:ed1', '1'
> create 'training:eagg', 1, 'cf1:ed2', '2'
> put 'training:eagg', 'cf1

 

